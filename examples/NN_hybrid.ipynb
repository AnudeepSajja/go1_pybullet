{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "# Read and combine CSV files\n",
    "file_paths = [\n",
    "    '/home/jovyan/userdata/ssajja2s/data_files/data_with_images/go1_images_with_data_01.csv',\n",
    "    '/home/jovyan/userdata/ssajja2s/data_files/data_with_images/go1_images_with_data_02.csv'\n",
    "]\n",
    "dfs = [pd.read_csv(file_path) for file_path in file_paths]\n",
    "df_combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "# Split observation state data and output (tau_data)\n",
    "obs_state_data = df_combined.iloc[:, :-12].drop('time', axis=1)\n",
    "tau_data = df_combined.iloc[:, -12:]\n",
    "\n",
    "# Further split observation state data into input and output\n",
    "out_d = obs_state_data.iloc[:, :10]  # First 10 columns\n",
    "in_d = obs_state_data.iloc[:, 10:]  # Remaining columns\n",
    "\n",
    "# Load and preprocess images from multiple directories\n",
    "image_dirs = [\n",
    "    '/home/jovyan/userdata/ssajja2s/data_files/images_trj1',\n",
    "    '/home/jovyan/userdata/ssajja2s/data_files/images_trj2'\n",
    "]\n",
    "\n",
    "# Collect all image file paths\n",
    "image_files = []\n",
    "for image_dir in image_dirs:\n",
    "    image_files.extend(sorted(os.listdir(image_dir)))\n",
    "\n",
    "image_size = (200, 200)\n",
    "images = []\n",
    "\n",
    "print(f\"Number of image files: {len(image_files)}\")\n",
    "print(f\"Input shape: {in_d.shape}\")\n",
    "print(f\"Output shape: {out_d.shape}\")\n",
    "\n",
    "# Process and resize images\n",
    "for image_dir in image_dirs:\n",
    "    for image_file in sorted(os.listdir(image_dir)):\n",
    "        img_path = os.path.join(image_dir, image_file)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = img.resize(image_size)\n",
    "        img_array = np.array(img)\n",
    "        images.append(img_array)\n",
    "\n",
    "# Convert images list to NumPy array\n",
    "images_data = np.array(images)\n",
    "\n",
    "# Verify the shapes of images, input, and output data\n",
    "print(f\"Images shape: {images_data.shape}\")  # Expected: (num_images, 200, 200, 3)\n",
    "\n",
    "# Convert observation state data to NumPy arrays\n",
    "in_d = in_d.to_numpy()\n",
    "out_d = out_d.to_numpy()\n",
    "\n",
    "# Convert all data to PyTorch tensors\n",
    "images_data = torch.tensor(images_data, dtype=torch.float32)\n",
    "in_d = torch.tensor(in_d, dtype=torch.float32)\n",
    "out_d = torch.tensor(out_d, dtype=torch.float32)\n",
    "\n",
    "# Normalize image tensor\n",
    "images_data = images_data / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Ensure images match the dataset\n",
    "assert len(images) == len(image_files), \"Mismatch between processed images and file count.\"\n",
    "\n",
    "# Create image indices for all observations\n",
    "observations_per_image = len(df_combined) // len(image_files)\n",
    "image_indices = [min(i // observations_per_image, len(image_files) - 1) for i in range(len(df_combined))]\n",
    "\n",
    "# Verify correctness of image indices\n",
    "assert len(image_indices) == len(df_combined), \"Mismatch in lengths of image_indices and dataset\"\n",
    "assert max(image_indices) < len(image_files), \"Index out of bounds in image_indices\"\n",
    "assert min(image_indices) >= 0, \"Negative index found in image_indices\"\n",
    "\n",
    "# Print final checks\n",
    "print(f\"Final Images tensor shape: {images_data.shape}\")\n",
    "print(f\"Input tensor shape: {in_d.shape}\")\n",
    "print(f\"Output tensor shape: {out_d.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class HybridDataset(Dataset):\n",
    "    def __init__(self, images, observations, states, image_indices, transform=None):\n",
    "        self.images = images\n",
    "        self.observations = observations\n",
    "        self.states = states\n",
    "        self.image_indices = image_indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_idx = self.image_indices[idx]  # Map observation index to image index\n",
    "        image = self.images[image_idx]\n",
    "        observation = self.observations[idx]\n",
    "        state = self.states[idx]\n",
    "\n",
    "        # Change image format from (H, W, C) to (C, H, W)\n",
    "        image = image.permute(2, 0, 1)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, observation, state\n",
    "\n",
    "# Define transformations for image normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    in_d, out_d, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Split image indices for train and test datasets\n",
    "train_image_indices = image_indices[:len(X_train)]\n",
    "test_image_indices = image_indices[len(X_train):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test datasets\n",
    "train_dataset = HybridDataset(images_data, X_train, y_train, train_image_indices, transform=transform)\n",
    "test_dataset = HybridDataset(images_data, X_test, y_test, test_image_indices, transform=transform)\n",
    "\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HybridNN class\n",
    "class HybridNN(nn.Module):\n",
    "    def __init__(self, input_obs_size=34, output_size=10):\n",
    "        super(HybridNN, self).__init__()\n",
    "\n",
    "        # Image Network: 3 ConvNet branches\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4, padding=3)\n",
    "        self.conv2 = nn.Conv2d(3, 32, kernel_size=8, stride=2, padding=3)\n",
    "        self.conv3 = nn.Conv2d(3, 32, kernel_size=8, stride=1, padding=3)\n",
    "\n",
    "        # Dynamically calculate flattened size\n",
    "        sample_input = torch.zeros(1, 3, 200, 200)  # Dummy input (batch_size=1, 3 channels, 200x200)\n",
    "        conv1_out = self.conv1(sample_input)\n",
    "        conv2_out = self.conv2(sample_input)\n",
    "        conv3_out = self.conv3(sample_input)\n",
    "        \n",
    "        self.conv_out_dim = (\n",
    "            torch.flatten(conv1_out, 1).shape[1] + \n",
    "            torch.flatten(conv2_out, 1).shape[1] + \n",
    "            torch.flatten(conv3_out, 1).shape[1]\n",
    "        )\n",
    "\n",
    "        # Fully connected layers for image features\n",
    "        self.fc1 = nn.Linear(self.conv_out_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "\n",
    "        # Observation Network\n",
    "        self.obs_fc1 = nn.Linear(input_obs_size, 64)\n",
    "        self.obs_fc2 = nn.Linear(64, 64)\n",
    "\n",
    "        # Combined Network\n",
    "        self.fc_combined = nn.Linear(64 + 64, 128)\n",
    "        self.output_layer = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, image, observation):\n",
    "        # Image processing\n",
    "        x1 = torch.relu(self.conv1(image))\n",
    "        x1 = torch.flatten(x1, 1)\n",
    "\n",
    "        x2 = torch.relu(self.conv2(image))\n",
    "        x2 = torch.flatten(x2, 1)\n",
    "\n",
    "        x3 = torch.relu(self.conv3(image))\n",
    "        x3 = torch.flatten(x3, 1)\n",
    "\n",
    "        image_features = torch.cat([x1, x2, x3], dim=1)\n",
    "        image_features = torch.relu(self.fc1(image_features))\n",
    "        image_features = torch.relu(self.fc2(image_features))\n",
    "\n",
    "        # Observation processing\n",
    "        obs_features = torch.relu(self.obs_fc1(observation))\n",
    "        obs_features = torch.relu(self.obs_fc2(obs_features))\n",
    "\n",
    "        # Combine image and observation features\n",
    "        combined_features = torch.cat([image_features, obs_features], dim=1)\n",
    "        x = torch.relu(self.fc_combined(combined_features))\n",
    "\n",
    "        # Output layer\n",
    "        output = self.output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = HybridNN(input_obs_size=in_d.shape[1], output_size=out_d.shape[1]).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SummaryWriter to log data for TensorBoard\n",
    "writer = SummaryWriter('runs/test_image_data')\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (images, observations, states) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        images = images.to(device)\n",
    "        observations = observations.to(device)\n",
    "        states = states.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, observations)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, states)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, observations, states in test_loader:\n",
    "            images = images.to(device)\n",
    "            observations = observations.to(device)\n",
    "            states = states.to(device)\n",
    "\n",
    "            outputs = model(images, observations)\n",
    "            loss = criterion(outputs, states)\n",
    "            running_test_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = running_test_loss / len(test_loader)\n",
    "    writer.add_scalar('Loss/val', avg_test_loss, epoch)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "model.eval()\n",
    "\n",
    "# Define paths for evaluation data and images\n",
    "eval_path = '/home/jovyan/userdata/ssajja2s/data_files/eval_data/go1_images_with_data_e.csv'\n",
    "eval_image_path = '/home/jovyan/userdata/ssajja2s/data_files/images_e'\n",
    "\n",
    "# Load evaluation data\n",
    "eval_data = pd.read_csv(eval_path)\n",
    "eval_obs_state_data = eval_data.iloc[:, :-12].drop('time', axis=1)\n",
    "eval_tau_data = eval_data.iloc[:, -12:]\n",
    "\n",
    "# Further split observation state data into input and output\n",
    "eval_out_d = eval_obs_state_data.iloc[:, :10]  # First 10 columns\n",
    "eval_in_d = eval_obs_state_data.iloc[:, 10:]  # Remaining columns\n",
    "\n",
    "# Load and preprocess evaluation images\n",
    "eval_images = []\n",
    "for image_file in sorted(os.listdir(eval_image_path)):\n",
    "    img_path = os.path.join(eval_image_path, image_file)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = img.resize(image_size)\n",
    "    img_array = np.array(img)\n",
    "    eval_images.append(img_array)\n",
    "\n",
    "# Convert evaluation images list to NumPy array\n",
    "eval_images_data = np.array(eval_images)\n",
    "\n",
    "# Convert evaluation observation state data to NumPy arrays\n",
    "eval_in_d = eval_in_d.to_numpy()\n",
    "eval_out_d = eval_out_d.to_numpy()\n",
    "\n",
    "# Convert all evaluation data to PyTorch tensors\n",
    "eval_images_data = torch.tensor(eval_images_data, dtype=torch.float32)\n",
    "eval_in_d = torch.tensor(eval_in_d, dtype=torch.float32)\n",
    "eval_out_d = torch.tensor(eval_out_d, dtype=torch.float32)\n",
    "\n",
    "# Normalize evaluation image tensor\n",
    "eval_images_data = eval_images_data / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Ensure evaluation images match the dataset\n",
    "assert len(eval_images) == len(os.listdir(eval_image_path)), \"Mismatch between processed evaluation images and file count.\"\n",
    "\n",
    "# Create image indices for all evaluation observations\n",
    "eval_observations_per_image = len(eval_data) // len(os.listdir(eval_image_path))\n",
    "eval_image_indices = [min(i // eval_observations_per_image, len(os.listdir(eval_image_path)) - 1) for i in range(len(eval_data))]\n",
    "\n",
    "# Verify correctness of evaluation image indices\n",
    "assert len(eval_image_indices) == len(eval_data), \"Mismatch in lengths of evaluation image_indices and dataset\"\n",
    "assert max(eval_image_indices) < len(os.listdir(eval_image_path)), \"Index out of bounds in evaluation image_indices\"\n",
    "assert min(eval_image_indices) >= 0, \"Negative index found in evaluation image_indices\"\n",
    "\n",
    "# Print final checks\n",
    "print(f\"Final Evaluation Images tensor shape: {eval_images_data.shape}\")\n",
    "print(f\"Evaluation Input tensor shape: {eval_in_d.shape}\")\n",
    "print(f\"Evaluation Output tensor shape: {eval_out_d.shape}\")\n",
    "\n",
    "# Create evaluation dataset\n",
    "eval_dataset = HybridDataset(eval_images_data, eval_in_d, eval_out_d, eval_image_indices, transform=transform)\n",
    "\n",
    "# Define batch size for evaluation\n",
    "eval_batch_size = 16\n",
    "\n",
    "# Create DataLoader for evaluation dataset\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=eval_batch_size, shuffle=False)\n",
    "\n",
    "# Create a new SummaryWriter for evaluation logs\n",
    "eval_writer = SummaryWriter('runs/test_image_data')\n",
    "\n",
    "# Evaluation loop and log the results for model predictions to actual values \n",
    "# Evaluation loop and log the results for model predictions to actual values \n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for batch_idx, (images, observations, states) in enumerate(eval_loader):\n",
    "        # Move data to device\n",
    "        images = images.to(device)\n",
    "        observations = observations.to(device)\n",
    "        states = states.to(device)\n",
    "\n",
    "        # Forward pass to get predictions\n",
    "        outputs = model(images, observations)\n",
    "\n",
    "        # Convert to NumPy for logging\n",
    "        actual_states = states.detach().cpu().numpy()\n",
    "        predicted_states = outputs.detach().cpu().numpy()\n",
    "\n",
    "        num_states = predicted_states.shape[1]  # Get number of states (e.g., joints)\n",
    "\n",
    "        state_names = [\n",
    "            'base_pos_x', 'base_pos_y', 'base_pos_z',\n",
    "            'base_ori_x', 'base_ori_y', 'base_ori_z', 'base_ori_w',\n",
    "            'base_vel_x', 'base_vel_y', 'base_vel_z'\n",
    "        ]\n",
    "\n",
    "        # Log actual and predicted values for each state\n",
    "        for i in range(num_states):\n",
    "            eval_writer.add_scalars(state_names[i], {\n",
    "                'Actual value': actual_states[:, i],  \n",
    "                'Predicted value': predicted_states[:, i]  # Use predictions directly for logging\n",
    "            }, global_step=batch_idx)\n",
    "\n",
    "# Close the SummaryWriter after evaluation\n",
    "eval_writer.close()\n",
    "\n",
    "print(\"Evaluation completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

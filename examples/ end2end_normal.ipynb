{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 20:34:04.078653: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-27 20:34:04.126687: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-27 20:34:05.012135: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 44)\n",
      "(15000, 12)\n"
     ]
    }
   ],
   "source": [
    "file_paths = [\n",
    "    '/home/anudeep/devel/workspace/src/data/go1_eval/go1_trot_data_eval.csv'\n",
    "]\n",
    "\n",
    "dfs = [pd.read_csv(file_path) for file_path in file_paths]\n",
    "df_combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "X = df_combined.iloc[:, :-12]\n",
    "Y = df_combined.iloc[:, -12:]\n",
    "\n",
    "X = X.drop('time', axis=1)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the last 4 columns \n",
    "X_last_4 = X.iloc[:, -4:]\n",
    "\n",
    "# scale the data to be between -1 and 1\n",
    "\n",
    "X_min = X.iloc[:, :-4].min(axis= 0)\n",
    "X_max = X.iloc[:, :-4].max(axis= 0)\n",
    "Y_min = Y.min(axis= 0)\n",
    "Y_max = Y.max(axis= 0)\n",
    "\n",
    "X_norm = X.iloc[:, :-4]\n",
    "\n",
    "X_norm = 2 * (X_norm - X_min) / (X_max - X_min) - 1\n",
    "Y_norm = 2 * (Y - Y_min) / (Y_max - Y_min) - 1\n",
    "\n",
    "X_input = pd.concat([X_norm, X_last_4], axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14477/2629748799.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  estimator = torch.load(model_path, map_location=device)  # Load model to the correct device\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jovyan/userdata/ssajja2s/models/end2end_predictor.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jovyan/userdata/ssajja2s/models/end2end_predictor.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load model to the correct device\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load the input data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# data_path = \"/home/jovyan/userdata/ssajja2s/data_files/eval_data/go1_trot_data_eval.csv\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jovyan/userdata/ssajja2s/data_files/new_data/go1_trot_data_06.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jovyan/userdata/ssajja2s/models/end2end_predictor.pth'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from END2ENDPredictor import NMPCPredictor \n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model\n",
    "model_path = \"/home/jovyan/userdata/ssajja2s/models/end2end_predictor.pth\"\n",
    "estimator = torch.load(model_path, map_location=device)  # Load model to the correct device\n",
    "\n",
    "# Load the input data\n",
    "# data_path = \"/home/jovyan/userdata/ssajja2s/data_files/eval_data/go1_trot_data_eval.csv\"\n",
    "data_path = \"/home/jovyan/userdata/ssajja2s/data_files/new_data/go1_trot_data_06.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Prepare input and output data\n",
    "X = df.iloc[:, :-12]\n",
    "Y = df.iloc[:, -12:]\n",
    "\n",
    "# Drop the first column 'time'\n",
    "X = X.drop('time', axis=1)\n",
    "\n",
    "# Store the last 4 columns \n",
    "X_last_4 = X.iloc[:, -4:]\n",
    "\n",
    "# Scale the data to be between -1 and 1\n",
    "X_min = X.iloc[:, :-4].min(axis=0)\n",
    "X_max = X.iloc[:, :-4].max(axis=0)\n",
    "Y_min = Y.min(axis=0)\n",
    "Y_max = Y.max(axis=0)\n",
    "\n",
    "X_norm = X.iloc[:, :-4]\n",
    "X_norm = 2 * (X_norm - X_min) / (X_max - X_min) - 1\n",
    "\n",
    "X_input = pd.concat([X_norm, X_last_4], axis=1)\n",
    "\n",
    "input_data = X_input.iloc[:, 2:] \n",
    "\n",
    "# Convert to tensor and move to device\n",
    "X_tensor = torch.tensor(input_data.values, dtype=torch.float32).to(device)\n",
    "\n",
    "# Get predictions\n",
    "estimator.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_torques = estimator(X_tensor)\n",
    "\n",
    "predicted_torques_numpy = predicted_torques.cpu().numpy()\n",
    "\n",
    "# Unnormalize predicted torques\n",
    "Y_max_tensor = torch.tensor(Y_max.values, dtype=torch.float32).to(device)\n",
    "Y_min_tensor = torch.tensor(Y_min.values, dtype=torch.float32).to(device)\n",
    "\n",
    "predicted_torques_unnormalized = ((torch.tensor(predicted_torques_numpy, dtype=torch.float32).to(device) + 1) * (Y_max_tensor - Y_min_tensor)) / 2 + Y_min_tensor\n",
    "\n",
    "# Store actual torques for comparison\n",
    "actual_torques_numpy = Y.to_numpy()\n",
    "\n",
    "# Log actual and predicted torques for each joint at each time step\n",
    "writer = SummaryWriter('runs/end2end')\n",
    "\n",
    "num_joints = predicted_torques_numpy.shape[1]  # Get number of joints from predictions\n",
    "num_time_steps = predicted_torques_numpy.shape[0]  # Get number of time steps\n",
    "\n",
    "for t in range(num_time_steps):\n",
    "    for i in range(num_joints):\n",
    "        writer.add_scalars(f'Joint {i+1}', {\n",
    "            'Actual Torque': actual_torques_numpy[t, i],  \n",
    "            'Predicted Torque': predicted_torques_unnormalized[t, i].item()  # Convert tensor to scalar for logging\n",
    "        }, global_step=t)  \n",
    "\n",
    "writer.close()  # Close the writer when done\n",
    "\n",
    "print(\"Predictions made and logged to TensorBoard.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator Evaluation\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from NMPCPredictor import NMPCPredictor  # Ensure this path is correct\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model checkpoint\n",
    "# model_path = '/home/jovyan/userdata/ssajja2s/models/end2end_predictor.pth'\n",
    "model_path = '/home/jovyan/userdata/ssajja2s/models/estimator_500_epoch.pth'\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# Initialize the model (ensure input and output sizes are correct)\n",
    "input_size = checkpoint['model_state_dict']['hidden1.weight'].shape[1]\n",
    "output_size = checkpoint['model_state_dict']['output.weight'].shape[0]\n",
    "neurons = 2560  # or whatever value you used during training\n",
    "\n",
    "# Initialize estimator with neurons parameter\n",
    "estimator = NMPCPredictor(input_size=input_size, output_size=output_size, neurons=neurons).to(device)\n",
    "\n",
    "# Load the state dict into the model\n",
    "estimator.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Load normalization parameters\n",
    "X_min = checkpoint['x_min']\n",
    "X_max = checkpoint['x_max']\n",
    "Y_min = checkpoint['y_min']\n",
    "Y_max = checkpoint['y_max']\n",
    "\n",
    "print(\"Model loaded successfully along with normalization parameters.\")\n",
    "\n",
    "# Load the input data\n",
    "# data_path = \"/home/jovyan/userdata/ssajja2s/data_files/new_data/go1_trot_data_06.csv\"\n",
    "# data_path = \"/home/jovyan/userdata/ssajja2s/data_files/eval_data/go1_trot_data_eval_test.csv\"\n",
    "# data_path =\"/home/jovyan/userdata/ssajja2s/data_files/eval_data/go1_trot_data_eval.csv\"\n",
    "data_path = \"/home/jovyan/userdata/ssajja2s/data_files/eval_data/go1_trot_eval_data_v3.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare input and output data\n",
    "obs_state_data = df.iloc[:, :-12]\n",
    "tau_data = obs_state_data.iloc[:, -12:]\n",
    "\n",
    "\n",
    "# Drop the first column 'time'\n",
    "obs_state_data = obs_state_data.drop('time', axis=1)\n",
    "\n",
    "Y = obs_state_data.iloc[:, :10]\n",
    "X = obs_state_data.iloc[:, 10:]\n",
    "\n",
    "\n",
    "# Store the last 4 columns \n",
    "X_last_4 = X.iloc[:, -4:]\n",
    "\n",
    "# Scale the data to be between -1 and 1\n",
    "X_norm = X.iloc[:, :-4]\n",
    "X_norm = 2 * (X_norm - X_min) / (X_max - X_min) - 1\n",
    "\n",
    "X_input = pd.concat([X_norm, X_last_4], axis=1)\n",
    "\n",
    "input_data = X_input\n",
    "\n",
    "\n",
    "# Convert to tensor and move to device\n",
    "X_tensor = torch.tensor(input_data.values, dtype=torch.float32).to(device)\n",
    "\n",
    "# Get predictions\n",
    "estimator.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_states = estimator(X_tensor)\n",
    "\n",
    "predicted_states_numpy = predicted_states.cpu().numpy()\n",
    "\n",
    "# Ensure Y_max and Y_min are NumPy arrays\n",
    "Y_max = np.array(Y_max)\n",
    "Y_min = np.array(Y_min)\n",
    "\n",
    "\n",
    "# Adjusted unnormalization formula using loaded normalization parameters\n",
    "predicted_states_unnormalized = ((predicted_states_numpy + 1) / 2) * (Y_max.reshape(1, -1) - Y_min.reshape(1, -1)) + Y_min.reshape(1, -1)\n",
    "\n",
    "# Store actual torques for comparison\n",
    "actual_states_numpy = Y.to_numpy()\n",
    "\n",
    "# Log actual and predicted torques for each joint at each time step\n",
    "writer = SummaryWriter('runs/estimator')\n",
    "\n",
    "num_states = predicted_states_unnormalized.shape[1]  # Get number of joints from predictions\n",
    "num_time_steps = predicted_states_unnormalized.shape[0]  # Get number of time steps\n",
    "\n",
    "state_names = [\n",
    "    'base_pos_x', 'base_pos_y', 'base_pos_z',\n",
    "    'base_ori_x', 'base_ori_y', 'base_ori_z', 'base_ori_w',\n",
    "    'base_vel_x', 'base_vel_y', 'base_vel_z'\n",
    "]\n",
    "\n",
    "for t in range(num_time_steps):\n",
    "    for i in range(num_states):\n",
    "        writer.add_scalars(state_names[i], {\n",
    "            'Actual value': actual_states_numpy[t, i],  \n",
    "            'Predicted value': predicted_states_unnormalized[t, i]  # Use unnormalized predictions for logging\n",
    "        }, global_step=t)  \n",
    "\n",
    "writer.close()  # Close the writer when done\n",
    "\n",
    "print(\"Predictions made and logged to TensorBoard.\")\n",
    "\n",
    "\n",
    "# Save predicted torques to a CSV file\n",
    "predicted_states_df = pd.DataFrame(predicted_states_unnormalized, columns=state_names)\n",
    "predicted_states_df.to_csv('predicted_states.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = obs_state_data.iloc[:, :10]\n",
    "Y = tau_data\n",
    "\n",
    "# remove pose_x and pose_y and for X which are first two columns\n",
    "X = X.iloc[:, 2:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
